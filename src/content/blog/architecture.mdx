---
title: 'Designing a Modular Architecture for Real-Time Racing Telemetry'
description: 'Placeholder'
pubDate: 2025-11-12
author: 'Ashwin Natarajan'
authorDescription: 'Developer'
category: 'Development'
image: '../../blog_assets/capturing-replaying-data/thumbnail.png'
tags: ['architecture', 'python', 'telemetry', 'socket.io', 'asyncio', 'qt']
---

Modern racing telemetry systems need to handle live data at high frequency, distribute it to multiple user interfaces, and maintain responsiveness across subsystems.
This post explains the architectural design behind a modular, multi-process telemetry platform that powers a real-time racing analysis environment.

---

## Overview

The system is designed around three major goals:

1. **High-frequency data handling** from a simulator without blocking UI updates.
2. **Separation of concerns** using isolated subsystems for core telemetry, HUD, and save file viewing.
3. **Resilient process management** through a central launcher that supervises all components.

At a glance, the architecture looks like this:

> **[PLACEHOLDER: insert architecture diagram image here]**

---

## Core Components

### 1. Process Management Layer

A dedicated **Launcher** oversees three key processes:

- **Backend (Main Server)** - responsible for telemetry ingestion, state computation, and API serving.
- **HUD Subsystem** - manages graphical overlays for in-race information.
- **Save Viewer Subsystem** - handles saved race data visualization and analysis.

Each process communicates with the launcher using lightweight **IPC channels** for heartbeats, control, and graceful restarts.

> **[PLACEHOLDER: image showing process manager structure]**

---

## Backend Architecture

The **Backend** is the foundation of the entire system. It's organized into three distinct layers for clarity and maintainability.

### Telemetry Layer

This layer listens to the **F1 Simulator's UDP stream** and converts raw packets into structured telemetry data.

- **UDP Receiver** - captures incoming packets with minimal latency.
- **Parsers** - convert simulator data into typed objects for internal use.
- **Packet Forwarder** - optionally forwards packets to external targets for debugging or analysis.

> **[PLACEHOLDER: telemetry layer diagram]**

### Interface Layer

This layer exposes real-time data and APIs to other modules and clients.

- **HTTP Server** - serves web-based dashboards and endpoints.
- **Socket.IO** - streams live telemetry to connected dashboards.
- **ZMQ IPC Server** - provides efficient inter-process communication between backend and subsystems.

This separation allows web clients, HUDs, and external apps to consume the same live data concurrently.

### State Management Layer

All incoming data flows into a central **Shared State**, updated by the **Computation Engine**.
Derived metrics and aggregated statistics are computed here, and the **API Interface** acts as the entry point for all reads and writes.

This structure ensures a single source of truth for all connected systems.
The holy principle of this layer is that only CPU bound operations are permitted here. **Absolutely no I/O bound operations**
This helps ensure a lock free state layer, thereby keeping no network client waiting longer than necessary

> **[PLACEHOLDER: state management flow image]**

---

## HUD Subsystem

The **HUD Subsystem** provides real-time visual overlays for streamers and drivers.
It runs independently from the backend to ensure no frame drops during gameplay.

- **IPC Server** - receives commands and updates.
- **Socket.IO Client** - subscribes to backend feeds.
- **Qt Overlays** - render transparent, click-through widgets over the game window.

> **[PLACEHOLDER: screenshot or diagram of HUD overlay flow]**

---

## Save Viewer Subsystem

Designed for post-race analysis, the **Save Viewer** uses a similar architecture but focuses on historical data rather than live feeds.

- **HTTP Server** - serves pages for saved session viewing.
- **Socket.IO Server** - updates connected dashboards in real time.
- **State Manager** - stores parsed save file data and notifies the front-end when ready.

> **[PLACEHOLDER: diagram of Save Viewer interactions]**

---

## Frontend Clients

Multiple frontends consume the backend data:

- **Stream Overlay** - live widgets for broadcasting.
- **Driver Dashboard** - detailed lap and sector metrics.
- **Engineer Dashboard** - comparative analytics across drivers and stints.

All UIs are powered by **Socket.IO updates** and served via **HTTP** routes from the backend or the save viewer.

> **[PLACEHOLDER: image showing connected dashboards]**

---

## Inter-Process Communication

Each subsystem communicates through **ZMQ IPC channels**, which provide:

- Non-blocking message passing
- Automatic reconnection
- Separation between user interfaces and data logic

This ensures each component can restart or crash independently without bringing down the others.

> **[PLACEHOLDER: IPC message flow diagram]**

---

## Reliability and Supervision

The **Launcher** monitors process health through periodic heartbeats and log output redirection.
If any subsystem becomes unresponsive, it can restart it automatically while maintaining shared state integrity.

This supervision model provides resilience during long racing sessions or unexpected load spikes.

> **[PLACEHOLDER: launcher supervision illustration]**

---

## Key Design Principles

1. **Isolation** - each process runs independently for modularity and fault tolerance.
2. **Extensibility** - new UI clients or telemetry parsers can be added without major refactoring.
3. **Performance** - asynchronous networking (using `asyncio`, `Socket.IO`, and `ZMQ`) ensures low latency.
4. **Maintainability** - consistent layering across backend, HUD, and save viewer simplifies debugging.

---

## Closing Thoughts

This architecture balances real-time performance with modularity.
By decoupling telemetry, visualization, and control logic into isolated processes, the system can handle demanding live data flows while remaining flexible for future extensions.

> **[PLACEHOLDER: final system overview image]**
